1. State of the Art Parser

Both the Stanford and Berkeley parsers seemed to be fooled by the sentence,

"Why did you bring that table that the book was on up?"

which contains center embedding and ends in two prepositions. They both parsed
the sentence in the following way.

(ROOT
  (SBARQ
    (WHADVP (WRB Why))
    (SQ (VBD did)
      (NP (PRP you))
      (VP (VB bring)
        (NP
          (NP (DT that) (NN table))
          (SBAR (IN that)
            (S
              (NP (DT the) (NN book))
              (VP (VBD was)
                (ADVP (RB on) (RB up))))))))
    (. ?)))

Clearly, however, "on up" is not an adverb phrase in this context, but rather
on was a preposition refering to the table.

Interestingly, the sentence,

"Is he okay, the person that fell?"

was parsed incorrectly by the Stanford parser, but parsed correctly by the
Berkeley parser. The Stanford parser produced,

(ROOT
  (SQ (VBZ Is)
    (NP (PRP he))
    (NP
      (NP
        (NP (NNP okay))
        (, ,)
        (NP (DT the) (NN person)))
      (SBAR
        (WHNP (WDT that))
        (S
          (VP (VBD fell)))))
    (. ?)))

which is incorrect in that "okay, person that fell" is not a noun phrase. The
Berkeley parser produced,

(ROOT
  (SQ (VBZ Is)
    (NP (PRP he))
    (ADJP (JJ okay))
    (, ,)
    (NP
      (NP (DT the) (NN person))
      (SBAR
        (WHNP (WDT that))
        (S
          (VP (VBD fell)))))
    (. ?)))

which seems to us to be the correct parse. Similarly, the Stanford parser
parsed the reverse sentence, 

"The person that fell, is he okay?"

incorrectly as well, while the Berkeley parser parsed correctly. Stanford's output was 

(ROOT
  (SQ
    (S
      (NP
        (NP (DT The) (NN person))
        (NP (DT that)))
      (VP (VBD fell)))
    (, ,)
    (SQ (VBZ is)
      (NP (PRP he))
      (ADJP (JJ okay)))
    (. ?)))

which is incorrect because "The person that" is not a noun phrase. Berkeley,
however, correctly identified the that clause with the following parse.

(ROOT
  (S
    (NP
      (NP (DT The) (NN person))
      (SBAR
        (WHNP (WDT that))
        (S
          (VP (VBD fell)))))
    (, ,)
    (SQ (VBZ is)
      (NP (PRP he))
      (ADJP (JJ okay))
      (. ?))))

Again quite interestingly, the sentence 

"A photo tour of one of the largest arcades in the world!"

grabbed from a Reddit post title is correctly identified as a fragment by the
Berkeley parser.

(ROOT
  (FRAG
    (NP
      (NP (DT A) (JJ photo) (NN tour))
      (PP (IN of)
        (NP
          (NP (CD one))
          (PP (IN of)
            (NP
              (NP (DT the) (JJS largest) (NNS arcades))
              (PP (IN in)
                (NP (DT the) (NN world))))))))
    (. !)))

I've never thought about how something looks like. 
vs
I've never thought about how something looks. 

Also words like "gimme" don't seem to fool the parser.

2.

(a)
We are able to ensure that our algorithm is within the given time and space
constraints by making sure that we use the correct datastructures for quick
lookup times. We use a hash table as a set to determine if a "state" or entry
is already in a particular column.

(b)
Adding to the column is simply an append on a Python list and an insertion into
a Python dictionary, both of which are O(1).

(c)
Before adding a new state or entry to a column we always check if that state is
already present. If it is already present, but has a lower weight change the
weight and backpointers of the state already in the column.

We are able to handle very general rules with this parser through careful design.
For example NP → NP . and NP causes us no problem because we make no assumptions
about the nature of each symbol on the right hand side. We simply march the dot
over to the right as we complete or scan constituents. To that end we consider
all terminal symbols like "and" as "free" or having zero weight and nonterminals
as having the weight of all of the rules that were used to make them.

We do in fact have the allowable bug. We did not have time to find a proper work
around for that bug.

We also implemented one optimization in this part of the assignment by default,
by only storing every state or entry of the form (3, X → · · · . CDE) as a 
single entry in each column. Specifically, we simply dropped, the symbols preceding
the dot, from the entry.

Please see "parse.py" for more details.

3.

In addition to the compression optimization that we implemented in part 2, we
implemented 2 major optimizations.

The first optimization that we implemented was the strongly recommended batch 
check for predicting a nonterminal symbol. The purpose of this is to check simply
if the nonterminal had been predicted before, for the current column. If it had
been we can just move on, instead of checking if each individual state is already 
in the column. If the grammar is large this can be a particularly large constant
saving. This was implemented by building a predicted symbol set for each column.
After predicting a nonterminal we added that symbol to the set and then we would
never predict that symbol again if it was in the set of previously predicted symbols.
Importantly, this optimization is perfectly compatible with the next optimization
because the word, w_j, in the jth column is constant so it's left ancestors will
not change.

The second optimization that we implemented was a left-corner optimization. Our
optimization was slightly different than the suggested optimization in the 
assignment. It works as follows. For each word in the sentence we build a set
of all symbols that are "left ancestors" of the word in the grammar. Left ancestor
means precisely that you can generate the word from the symbol by recurively 
following the grammars generation rules for the first symbol for any combination
of grammar rules. Given that we have such a set we can simply check if the first 
symbol of the rule we are about to predict is in the left ancestor set for the
word. If the first symbol is a left ancestor predict that rule, otherwise there
is no point in predicting that rule because it can never be completed. That is to
say there is no way of reaching the word from the first symbol in this rule.

We build this set by recursively exploring the left parents of word, and adding 
new parents to the set as long as we have not added them before. Recursively,
because once we explore the left parents of the word we recursively explore
the left parents of the left parents, and so forth.

This method is somewhat less complicated that the method described in the assignment
and as far as we can figure accomplishes exactly the same effect. Furthermore,
it reduces the memory overhead required, as we no longer need an prefix table as
required by the assignment's suggested method and additionaly the S table would 
contain many duplicates since S(A) would include all symbols in S(B) if we had the
rule A -> B ... where B is a left ancestor of the current word. While this would
likely be minimal since the sets are not storing duplicate objects it would not
be insignificant since making a table would probably require hash tables of hash
tables.

Indeed, our method allows for an addition optimization that we did not implement
wherein you can store these sets for each word, such that if there are repeated
words in the sentence you would not have to recalculate the set of left ancestors.

The result of these speed ups was that we could complete parsing wallstreet.sen
in about 48 seconds, which is more than twice the speed of the lisp implementation
and many many times faster than our original implementation, which takes quite a
while to process wallstreet.sen

Please see "parse2.py" for more details.

